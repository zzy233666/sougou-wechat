import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
import json
import os
import time
import random
import schedule
from typing import List, Dict, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import logging
from minio_storage import MinIOArticleStorage
from anti_crawler import create_anti_crawler_session


@dataclass
class WeChatArticle:
    """å¾®ä¿¡æ–‡ç« æ•°æ®ç»“æ„"""
    title: str = ""
    summary: str = ""
    source: str = ""
    publish_time: str = ""
    sogou_url: str = ""
    real_url: str = ""
    crawl_time: str = ""
    success: bool = False
    content: str = ""  # æ–‡ç« æ­£æ–‡å†…å®¹ï¼ˆçº¯æ–‡æœ¬ï¼‰
    content_fetched: bool = False  # æ˜¯å¦æˆåŠŸè·å–å†…å®¹

class WeChatCrawler:
    """å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«ç±» - å°è£…ç”¨äºFastAPIé›†æˆ"""
    
    def __init__(self, config_file: str = "wechat_accounts.txt", use_anti_crawler: bool = True):
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–é˜²åçˆ¬ç³»ç»Ÿ
        self.use_anti_crawler = use_anti_crawler
        
        # å§‹ç»ˆä¿ç•™headerså±æ€§ï¼Œç”¨äºå…¼å®¹æ€§
        self.headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "zh-CN,zh;q=0.9,ja;q=0.8",
            "Connection": "keep-alive",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36",
            "sec-ch-ua": '"Not;A=Brand";v="99", "Google Chrome";v="139", "Chromium";v="139"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
        }
        
        if self.use_anti_crawler:
            self.anti_crawler_session = create_anti_crawler_session(use_proxy=False, max_retries=3)
            self.logger.info("é˜²åçˆ¬ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        else:
            self.logger.info("ä½¿ç”¨ä¼ ç»Ÿè¯·æ±‚æ–¹å¼")
        
        # åˆå§‹åŒ–MinIOå­˜å‚¨
        self.storage = MinIOArticleStorage()
        self.logger.info("MinIOå­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
        
        # é…ç½®æ–‡ä»¶è·¯å¾„
        self.config_file = config_file
        
        
    def load_wechat_accounts(self) -> List[str]:
        """ä»é…ç½®æ–‡ä»¶åŠ è½½å¾®ä¿¡å…¬ä¼—å·åˆ—è¡¨"""
        accounts = []
        try:
            if not os.path.exists(self.config_file):
                self.logger.warning(f"é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return [
                    "ä¸­é‡‘æ‰€å‘å¸ƒ",
                    "ä¸Šäº¤æ‰€å‘å¸ƒ", 
                    "æè¿…é›·é‡‘èä¸æŠ•èµ„",
                    "é‡å­ä½",
                    "æœºå™¨ä¹‹å¿ƒ",
                    "è¯åˆ¸æ—¶æŠ¥",
                    "è´¢ç»æ—©é¤",
                    "ç•…æ¸¸è‚¡æµ·çš„è€èˆ¹é•¿",
                    "ç´¢ç­–ç•¥",
                    "è´¢å¤©æ—©çŸ¥é“",
                    "è¯ç›‘ä¼šå‘å¸ƒ",
                    "ä¸­è¯åå‘å¸ƒ",
                    "ä¸­å›½åŸºé‡‘æŠ¥",
                    "è“æ´æ–°æ¶ˆè´¹",
                    "ä¸Šæµ·è¯åˆ¸æŠ¥",
                    "21ä¸–çºªç»æµæŠ¥é“",
                    "åˆ¸å•†ä¸­å›½",
                    "ä¸­å›½è¯åˆ¸æŠ¥",
                    "é˜¿å°”æ³•å·¥åœºç ”ç©¶é™¢",
                    "é‡‘çŸ³æ‚è°ˆ",
                    "å®ç­–è‚¡",
                    "180K",
                    "è¯åˆ¸æ—¶æŠ¥è´¢å¯Œèµ„è®¯",
                    "éŸ­ç ”å…¬ç¤¾",
                    "è¡¨èˆ…æ˜¯å…»åŸºå¤§æˆ·",
                    "è¿œå·ç ”ç©¶æ‰€",
                    "æ ¼ä¸Šè´¢å¯Œ",
                    "çœŸæ˜¯æ¸¯è‚¡åœˆ",
                    "åå°”è¡—è§é—»",
                    "å¯»ç‘•è®°"
                ]
            
            with open(self.config_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                    if line and not line.startswith('#'):
                        accounts.append(line)
                        
            if accounts:
                self.logger.info(f"ä»é…ç½®æ–‡ä»¶åŠ è½½äº† {len(accounts)} ä¸ªå…¬ä¼—å·: {accounts}")
            else:
                self.logger.warning("é…ç½®æ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                accounts = [
                    "ä¸­é‡‘æ‰€å‘å¸ƒ",
                    "ä¸Šäº¤æ‰€å‘å¸ƒ", 
                    "æè¿…é›·é‡‘èä¸æŠ•èµ„",
                    "é‡å­ä½",
                    "æœºå™¨ä¹‹å¿ƒ",
                    "è¯åˆ¸æ—¶æŠ¥",
                    "è´¢ç»æ—©é¤",
                    "ç•…æ¸¸è‚¡æµ·çš„è€èˆ¹é•¿",
                    "ç´¢ç­–ç•¥",
                    "è´¢å¤©æ—©çŸ¥é“",
                    "è¯ç›‘ä¼šå‘å¸ƒ",
                    "ä¸­è¯åå‘å¸ƒ",
                    "ä¸­å›½åŸºé‡‘æŠ¥",
                    "è“æ´æ–°æ¶ˆè´¹",
                    "ä¸Šæµ·è¯åˆ¸æŠ¥",
                    "21ä¸–çºªç»æµæŠ¥é“",
                    "åˆ¸å•†ä¸­å›½",
                    "ä¸­å›½è¯åˆ¸æŠ¥",
                    "é˜¿å°”æ³•å·¥åœºç ”ç©¶é™¢",
                    "é‡‘çŸ³æ‚è°ˆ",
                    "å®ç­–è‚¡",
                    "180K",
                    "è¯åˆ¸æ—¶æŠ¥è´¢å¯Œèµ„è®¯",
                    "éŸ­ç ”å…¬ç¤¾",
                    "è¡¨èˆ…æ˜¯å…»åŸºå¤§æˆ·",
                    "è¿œå·ç ”ç©¶æ‰€",
                    "æ ¼ä¸Šè´¢å¯Œ",
                    "çœŸæ˜¯æ¸¯è‚¡åœˆ",
                    "åå°”è¡—è§é—»",
                    "å¯»ç‘•è®°"
                ]
                
        except Exception as e:
            self.logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            accounts = [
                "ä¸­é‡‘æ‰€å‘å¸ƒ",
                "ä¸Šäº¤æ‰€å‘å¸ƒ", 
                "æè¿…é›·é‡‘èä¸æŠ•èµ„",
                "é‡å­ä½",
                "æœºå™¨ä¹‹å¿ƒ",
                "è¯åˆ¸æ—¶æŠ¥",
                "è´¢ç»æ—©é¤",
                "ç•…æ¸¸è‚¡æµ·çš„è€èˆ¹é•¿",
                "ç´¢ç­–ç•¥",
                "è´¢å¤©æ—©çŸ¥é“",
                "è¯ç›‘ä¼šå‘å¸ƒ",
                "ä¸­è¯åå‘å¸ƒ",
                "ä¸­å›½åŸºé‡‘æŠ¥",
                "è“æ´æ–°æ¶ˆè´¹",
                "ä¸Šæµ·è¯åˆ¸æŠ¥",
                "21ä¸–çºªç»æµæŠ¥é“",
                "åˆ¸å•†ä¸­å›½",
                "ä¸­å›½è¯åˆ¸æŠ¥",
                "é˜¿å°”æ³•å·¥åœºç ”ç©¶é™¢",
                "é‡‘çŸ³æ‚è°ˆ",
                "å®ç­–è‚¡",
                "180K",
                "è¯åˆ¸æ—¶æŠ¥è´¢å¯Œèµ„è®¯",
                "éŸ­ç ”å…¬ç¤¾",
                "è¡¨èˆ…æ˜¯å…»åŸºå¤§æˆ·",
                "è¿œå·ç ”ç©¶æ‰€",
                "æ ¼ä¸Šè´¢å¯Œ",
                "çœŸæ˜¯æ¸¯è‚¡åœˆ",
                "åå°”è¡—è§é—»",
                "å¯»ç‘•è®°"
            ]
            
        return accounts
    
    def save_wechat_accounts(self, accounts: List[str]) -> bool:
        """ä¿å­˜å…¬ä¼—å·åˆ—è¡¨åˆ°é…ç½®æ–‡ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼Œä»…ç”¨äºåˆå§‹åŒ–ï¼‰"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                f.write("# å¾®ä¿¡å…¬ä¼—å·é…ç½®æ–‡ä»¶\n")
                f.write("# æ¯è¡Œä¸€ä¸ªå…¬ä¼—å·åç§°ï¼Œæ”¯æŒ#å·æ³¨é‡Š\n\n")
                
                for account in accounts:
                    f.write(f"{account}\n")
                    
            self.logger.info(f"å·²ä¿å­˜ {len(accounts)} ä¸ªå…¬ä¼—å·åˆ°é…ç½®æ–‡ä»¶")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def extract_real_url(self, response_text: str) -> Optional[str]:
        """ä»æœç‹—å¾®ä¿¡é‡å®šå‘é¡µé¢çš„JavaScriptä¸­æå–çœŸå®çš„å¾®ä¿¡æ–‡ç« URL"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…JavaScriptä¸­çš„URLæ„å»ºéƒ¨åˆ†
        url_pattern = r"url \+= '([^']+)';"
        matches = re.findall(url_pattern, response_text)
        
        if matches:
            # å°†æ‰€æœ‰åŒ¹é…çš„éƒ¨åˆ†æ‹¼æ¥æˆå®Œæ•´URL
            real_url = ''.join(matches)
            return real_url
        
        # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥åŒ¹é…å®Œæ•´çš„URLæ¨¡å¼
        full_url_pattern = r'https://mp\.weixin\.qq\.com/s\?[^"\']* '
        full_match = re.search(full_url_pattern, response_text)
        if full_match:
            return full_match.group(0)
        
        return None
    
    def get_real_wechat_url(self, sogou_url: str) -> Optional[str]:
        """è·å–æœç‹—å¾®ä¿¡é“¾æ¥å¯¹åº”çš„çœŸå®å¾®ä¿¡æ–‡ç« URL"""
        try:
            if self.use_anti_crawler:
                response = self.anti_crawler_session.get(sogou_url, timeout=10)
            else:
                response = requests.get(sogou_url, headers=self.headers, timeout=10)
            
            response.raise_for_status()
            real_url = self.extract_real_url(response.text)
            if real_url:
                self.logger.info(f"æˆåŠŸæå–çœŸå®URL: {real_url[:100]}...")
                return real_url
            else:
                self.logger.warning(f"æœªèƒ½æå–åˆ°çœŸå®URL: {sogou_url[:100]}...")
                return None
                
        except requests.RequestException as e:
            self.logger.error(f"è¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def extract_article_text(self, html_content: str) -> str:
        """ä»HTMLä¸­æå–æ–‡ç« æ­£æ–‡å†…å®¹"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
        for script in soup(["script", "style"]):
            script.decompose()
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥å®šä½æ–‡ç« æ­£æ–‡
        content_selectors = [
            '#js_content',  # å¾®ä¿¡æ–‡ç« ä¸»è¦å†…å®¹åŒºåŸŸ
            '.rich_media_content',  # å¾®ä¿¡æ–‡ç« å†…å®¹
            '.article-content',
            '.content',
            'article',
            '.post-content'
        ]
        
        content_text = ""
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # è·å–æ–‡æœ¬å†…å®¹å¹¶æ¸…ç†
                content_text = content_elem.get_text(separator='\n', strip=True)
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šçš„å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨bodyæ ‡ç­¾
        if not content_text:
            body = soup.find('body')
            if body:
                content_text = body.get_text(separator='\n', strip=True)
        
        # æ¸…ç†æ–‡æœ¬ï¼šç§»é™¤å¤šä½™çš„ç©ºè¡Œå’Œç©ºæ ¼
        lines = [line.strip() for line in content_text.split('\n') if line.strip()]
        clean_content = '\n'.join(lines)
        
        return clean_content
    
    def fetch_article_content(self, real_url: str, title: str = "") -> Dict[str, str]:
        """è·å–å¾®ä¿¡æ–‡ç« çš„æ­£æ–‡å†…å®¹"""
        try:
            if self.use_anti_crawler:
                # ä½¿ç”¨é˜²åçˆ¬ç³»ç»Ÿ
                response = self.anti_crawler_session.get(real_url, timeout=15)
            else:
                # ä¸ºå¾®ä¿¡æ–‡ç« è®¾ç½®ç‰¹æ®Šçš„è¯·æ±‚å¤´
                wechat_headers = self.headers.copy()
                wechat_headers.update({
                    "Referer": "https://weixin.sogou.com/",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
                })
                response = requests.get(real_url, headers=wechat_headers, timeout=15)
            
            response.raise_for_status()
            
            # æå–æ­£æ–‡å†…å®¹
            content_text = self.extract_article_text(response.text)
            
            if content_text:
                self.logger.info(f"æˆåŠŸæå–æ–‡ç« æ­£æ–‡å†…å®¹ï¼Œé•¿åº¦: {len(content_text)} å­—ç¬¦")
                return {
                    "content": content_text,
                    "success": True
                }
            else:
                self.logger.warning("æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„æ–‡ç« å†…å®¹")
                return {
                    "content": "",
                    "success": False
                }
            
        except requests.RequestException as e:
            self.logger.error(f"è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            return {
                "content": "",
                "success": False
            }
        except Exception as e:
            self.logger.error(f"æå–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            return {
                "content": "",
                "success": False
            }
    
    def search_articles(self, query: str, page: int = 1, start_time: Optional[datetime] = None, end_time: Optional[datetime] = None) -> List[WeChatArticle]:
        """æœç´¢å¾®ä¿¡æ–‡ç« """
        
        url = "https://weixin.sogou.com/weixin"
        params = {
            "query": query,
            "_sug_type_": "",
            "s_from": "input",
            "_sug_": "y",
            "type": "2",
            "page": str(page),
            "ie": "utf8"
        }
        
        # å¢å¼ºè¯·æ±‚å¤´
        enhanced_headers = self.headers.copy()
        enhanced_headers.update({
            "Accept-Encoding": "gzip, deflate, br",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin",
            "Cache-Control": "max-age=0",
            "Referer": "https://weixin.sogou.com/"
        })
        
        try:
            if self.use_anti_crawler:
                # ä½¿ç”¨é˜²åçˆ¬ç³»ç»Ÿ
                # å…ˆè®¿é—®é¦–é¡µå»ºç«‹ä¼šè¯
                self.anti_crawler_session.get("https://weixin.sogou.com/", timeout=10)
                time.sleep(random.uniform(2, 4))  # éšæœºå»¶è¿Ÿ
                
                response = self.anti_crawler_session.get(url, params=params, timeout=15)
            else:
                # å…ˆè®¿é—®é¦–é¡µå»ºç«‹ä¼šè¯
                requests.get("https://weixin.sogou.com/", headers=enhanced_headers, timeout=10)
                time.sleep(random.uniform(2, 4))  # éšæœºå»¶è¿Ÿ
                
                response = requests.get(url, headers=enhanced_headers, params=params, timeout=15)
            
            
            # è·å–cookies æ›´æ–°åˆ° headersï¼ˆä»…åœ¨ä¸ä½¿ç”¨é˜²åçˆ¬ç³»ç»Ÿæ—¶ï¼‰
            if not self.use_anti_crawler and response.cookies:
                cookies_str = '; '.join([f"{key}={value}" for key, value in response.cookies.items()])
                self.headers['Cookie'] = cookies_str
            response.raise_for_status()
            
            articles = self._parse_search_results(response.text, query)
            
            # å¦‚æœæŒ‡å®šäº†æ—¶é—´èŒƒå›´ï¼Œè¿›è¡Œè¿‡æ»¤
            if start_time or end_time:
                articles = self._filter_articles_by_time(articles, start_time, end_time)
            
            self.logger.info(f"æœç´¢ '{query}' ç¬¬{page}é¡µï¼Œæ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            return articles
            
        except requests.RequestException as e:
            self.logger.error(f"æœç´¢è¯·æ±‚å¤±è´¥: {e}")
            return []
    
    def _parse_search_results(self, html_content: str, query: str) -> List[WeChatArticle]:
        """è§£ææœç´¢ç»“æœé¡µé¢"""
        soup = BeautifulSoup(html_content, 'html.parser')
        articles = []
        
        # ç²¾å‡†å®šä½ï¼šæœç‹—å¾®ä¿¡æœç´¢ç»“æœåœ¨ ul.news-list ä¸‹çš„ li å…ƒç´ ä¸­
        news_items = soup.select('ul.news-list li')
        
        for item in news_items:
            article = WeChatArticle()
            article.crawl_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # 1. æå–æ ‡é¢˜
            title_elem = item.select_one('h3 a')
            if title_elem:
                article.title = title_elem.get_text(strip=True)
            else:
                title_elem = item.select_one('h3')
                if title_elem:
                    article.title = title_elem.get_text(strip=True)
            
            if not article.title:
                continue
            
            # 2. æå–ç®€è¦
            summary_elems = item.select('p')
            for p_elem in summary_elems:
                text = p_elem.get_text(strip=True)
                if (len(text) > 20 and 
                    not re.search(r'\d{4}-\d{1,2}-\d{1,2}', text) and
                    not re.search(r'ä»Šæ—¥|æ˜¨æ—¥|\d+å°æ—¶å‰|\d+åˆ†é’Ÿå‰', text) and
                    'å¾®ä¿¡å…¬ä¼—å¹³å°' not in text):
                    article.summary = text[:300] + '...' if len(text) > 300 else text
                    break
            
            # 3. æå–æœç‹—é“¾æ¥
            link_elem = item.select_one('h3 a')
            if link_elem:
                href = link_elem.get('href', '')
                if href:
                    if href.startswith('/'):
                        article.sogou_url = 'https://weixin.sogou.com' + href
                    else:
                        article.sogou_url = href
            
            # 4. æå–æ¥æº
            source_elem = item.select_one('div.s-p span.all-time-y2')
            if source_elem:
                source_text = source_elem.get_text(strip=True)
                if source_text and source_text != 'å¾®ä¿¡å…¬ä¼—å¹³å°':
                    article.source = source_text
            
            # 5. æå–æ—¶é—´
            time_script_elem = item.select_one('div.s-p span.s2 script')
            if time_script_elem:
                script_text = time_script_elem.get_text()
                timestamp_match = re.search(r'timeConvert\(\'(\d+)\'\)', script_text)
                if timestamp_match:
                    timestamp = int(timestamp_match.group(1))
                    article.publish_time = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
            
            articles.append(article)
        
        return articles
    
    def _filter_articles_by_time(self, articles: List[WeChatArticle], start_time: Optional[datetime] = None, end_time: Optional[datetime] = None) -> List[WeChatArticle]:
        """æ ¹æ®æ—¶é—´èŒƒå›´è¿‡æ»¤æ–‡ç« """
        if not start_time and not end_time:
            return articles
        
        filtered_articles = []
        
        for article in articles:
            if not article.publish_time:
                continue
                
            try:
                # è§£ææ–‡ç« å‘å¸ƒæ—¶é—´
                article_time = datetime.strptime(article.publish_time, '%Y-%m-%d %H:%M:%S')
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                in_range = True
                
                if start_time and article_time < start_time:
                    in_range = False
                    
                if end_time and article_time > end_time:
                    in_range = False
                
                if in_range:
                    filtered_articles.append(article)
                    
            except (ValueError, TypeError) as e:
                self.logger.warning(f"æ— æ³•è§£ææ–‡ç« æ—¶é—´: {article.publish_time}, é”™è¯¯: {e}")
                continue
        
        self.logger.info(f"æ—¶é—´è¿‡æ»¤ï¼šåŸæ–‡ç«  {len(articles)} ç¯‡ï¼Œè¿‡æ»¤å {len(filtered_articles)} ç¯‡")
        return filtered_articles
    
    def get_real_urls_batch(self, articles: List[WeChatArticle], max_workers: int = 3) -> List[WeChatArticle]:
        """æ‰¹é‡è·å–çœŸå®URL"""
        def process_article(article: WeChatArticle) -> WeChatArticle:
            if article.sogou_url:
                real_url = self.get_real_wechat_url(article.sogou_url)
                if real_url:
                    article.real_url = real_url
                    article.success = True
                else:
                    article.success = False
                # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(1)
            return article
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡è·å– {len(articles)} ç¯‡æ–‡ç« çš„çœŸå®URL...")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            processed_articles = list(executor.map(process_article, articles))
        
        successful_count = sum(1 for article in processed_articles if article.success)
        self.logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸè·å– {successful_count}/{len(articles)} ä¸ªçœŸå®URL")
        
        return processed_articles
    
    def save_article_to_storage(self, article: WeChatArticle) -> bool:
        """ä¿å­˜æ–‡ç« åˆ°MinIOå­˜å‚¨"""
        try:
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä»¥å…¼å®¹MinIOå­˜å‚¨
            article_dict = {
                "title": article.title,
                "summary": article.summary,
                "source": article.source,
                "publish_time": article.publish_time,
                "sogou_url": article.sogou_url,
                "real_url": article.real_url,
                "content": article.content,
                "crawl_time": article.crawl_time,
                "success": article.success,
                "content_fetched": article.content_fetched
            }
            
            # ä½¿ç”¨MinIOå­˜å‚¨ä¿å­˜å•ç¯‡æ–‡ç« 
            success = self.storage.save_article(article_dict)
            
            if success:
                self.logger.info(f"æ–‡ç« å·²ä¿å­˜åˆ°MinIO: {article.title[:30]}...")
                return True
            else:
                self.logger.warning(f"æ–‡ç« å¯èƒ½é‡å¤ï¼Œæœªä¿å­˜: {article.title[:30]}...")
                return False
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ç« åˆ°MinIOå¤±è´¥: {e}")
            return False
    
    def fetch_contents_batch(self, articles: List[WeChatArticle], max_workers: int = 2) -> List[WeChatArticle]:
        """æ‰¹é‡è·å–æ–‡ç« æ­£æ–‡å†…å®¹å¹¶ä¿å­˜åˆ°æ•°æ®åº“"""
        def fetch_content(article: WeChatArticle) -> WeChatArticle:
            if article.real_url and article.success:
                content_result = self.fetch_article_content(article.real_url, article.title)
                article.content = content_result["content"]
                article.content_fetched = content_result["success"]
                
                # ä¿å­˜åˆ°MinIOå­˜å‚¨
                if article.content_fetched:
                    storage_success = self.save_article_to_storage(article)
                    if storage_success:
                        self.logger.info(f"æ–‡ç« å·²ä¿å­˜åˆ°MinIOå­˜å‚¨: {article.title[:30]}...")
                
                # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(2)
            return article
        
        # åªå¤„ç†æˆåŠŸè·å–çœŸå®URLçš„æ–‡ç« 
        valid_articles = [article for article in articles if article.success and article.real_url]
        
        if not valid_articles:
            self.logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ç« URLå¯ä»¥è·å–å†…å®¹")
            return articles
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡è·å– {len(valid_articles)} ç¯‡æ–‡ç« çš„æ­£æ–‡å†…å®¹...")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # åªå¤„ç†æœ‰æ•ˆæ–‡ç« 
            processed_valid = list(executor.map(fetch_content, valid_articles))
        
        # æ›´æ–°åŸå§‹åˆ—è¡¨ä¸­çš„å¯¹åº”æ–‡ç« 
        valid_dict = {id(article): article for article in processed_valid}
        for i, article in enumerate(articles):
            if id(article) in valid_dict:
                articles[i] = valid_dict[id(article)]
        
        successful_content_count = sum(1 for article in articles if article.content_fetched)
        self.logger.info(f"å†…å®¹è·å–å®Œæˆï¼ŒæˆåŠŸè·å– {successful_content_count}/{len(valid_articles)} ç¯‡æ–‡ç« å†…å®¹")
        
        return articles
    
    def crawl_and_extract(self, query: str, page: int = 1, get_real_urls: bool = True, fetch_content: bool = False, start_time: Optional[datetime] = None, end_time: Optional[datetime] = None) -> Dict:
        """å®Œæ•´çš„çˆ¬å–å’Œæå–æµç¨‹"""
        start_time_exec = time.time()
        
        # 1. æœç´¢æ–‡ç« 
        articles = self.search_articles(query, page, start_time, end_time)
        
        if not articles:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç›¸å…³æ–‡ç« ",
                "data": [],
                "stats": {"total": 0, "real_urls_extracted": 0, "content_fetched": 0, "duration": 0}
            }
        
        # 2. è·å–çœŸå®URLï¼ˆå¯é€‰ï¼‰
        if get_real_urls:
            articles = self.get_real_urls_batch(articles)
        
        # 3. è·å–å®Œæ•´å†…å®¹ï¼ˆå¯é€‰ï¼‰
        if fetch_content and get_real_urls:
            articles = self.fetch_contents_batch(articles)
        
        # 4. ç»Ÿè®¡ç»“æœ
        total_articles = len(articles)
        successful_extractions = sum(1 for article in articles if article.success)
        content_fetched_count = sum(1 for article in articles if article.content_fetched)
        duration = time.time() - start_time_exec
        
        # 5. è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        articles_data = []
        for article in articles:
            articles_data.append({
                "title": article.title,
                "summary": article.summary,
                "source": article.source,
                "publish_time": article.publish_time,
                "sogou_url": article.sogou_url,
                "real_url": article.real_url,
                "crawl_time": article.crawl_time,
                "success": article.success,
                "content": article.content[:200] + "..." if len(article.content) > 200 else article.content,  # åªæ˜¾ç¤ºå‰200å­—ç¬¦
                "content_fetched": article.content_fetched
            })
        
        return {
            "success": True,
            "message": f"æˆåŠŸçˆ¬å– {total_articles} ç¯‡æ–‡ç« " + (f"ï¼Œè·å– {content_fetched_count} ç¯‡å®Œæ•´å†…å®¹" if fetch_content else ""),
            "data": articles_data,
            "stats": {
                "total": total_articles,
                "real_urls_extracted": successful_extractions,
                "content_fetched": content_fetched_count,
                "duration": round(duration, 2),
                "query": query,
                "page": page
            }
        }
    
    def crawl_all_configured_accounts(
        self,
        get_real_urls: bool = True,
        fetch_content: bool = False,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None
    ) -> Dict:
        """
        çˆ¬å–æ‰€æœ‰é…ç½®æ–‡ä»¶ä¸­çš„å…¬ä¼—å·ï¼ˆç®€åŒ–ç‰ˆï¼‰
        """
        accounts = self.load_wechat_accounts()
        
        if not accounts:
            return {
                "success": False,
                "message": "æ²¡æœ‰é…ç½®ä»»ä½•å…¬ä¼—å·",
                "data": [],
                "stats": {"queries": 0, "total_found": 0}
            }
        
        self.logger.info(f"å¼€å§‹çˆ¬å–é…ç½®æ–‡ä»¶ä¸­çš„ {len(accounts)} ä¸ªå…¬ä¼—å·")
        
        all_articles = []
        total_articles = 0
        
        for i, account in enumerate(accounts, 1):
            self.logger.info(f"[{i}/{len(accounts)}] æ­£åœ¨çˆ¬å–å…¬ä¼—å·ï¼š{account}")
            
            try:
                result = self.crawl_and_extract(
                    query=account,
                    page=1,
                    get_real_urls=get_real_urls,
                    fetch_content=fetch_content,
                    start_time=start_time,
                    end_time=end_time
                )
                
                if result['success']:
                    all_articles.extend(result['data'])
                    total_articles += len(result['data'])
                    self.logger.info(f"å…¬ä¼—å· '{account}' çˆ¬å–æˆåŠŸï¼š{len(result['data'])} ç¯‡æ–‡ç« ")
                else:
                    self.logger.error(f"å…¬ä¼—å· '{account}' çˆ¬å–å¤±è´¥ï¼š{result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if i < len(accounts):
                    time.sleep(2)
                    
            except Exception as e:
                self.logger.error(f"çˆ¬å–å…¬ä¼—å· '{account}' æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š{e}")
                continue
        
        return {
            'success': True,
            'data': all_articles,
            'stats': {
                'queries': len(accounts),
                'total_found': total_articles,
                'timestamp': datetime.now().isoformat()
            }
        }

    def get_articles_from_storage(self, limit: int = None) -> List[Dict]:
        """ä»MinIOå­˜å‚¨è·å–æ–‡ç« """
        try:
            # ä½¿ç”¨MinIOå­˜å‚¨æœç´¢æ–‡ç« 
            articles = self.storage.search_articles(limit=limit or 100)
            
            # æŒ‰æ—¶é—´æ’åº
            articles.sort(key=lambda x: x.get('crawl_time', ''), reverse=True)
            
            if limit:
                articles = articles[:limit]
            
            self.logger.info(f"ä»MinIOå­˜å‚¨è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            return articles
            
        except Exception as e:
            self.logger.error(f"ä»MinIOå­˜å‚¨è·å–æ–‡ç« å¤±è´¥: {e}")
            return []
    
    def save_results(self, results: Dict, filename: str = None) -> str:
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'wechat_articles_{timestamp}.json'
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        return filename
    
    def get_anti_crawler_stats(self) -> Dict:
        """è·å–é˜²åçˆ¬ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if self.use_anti_crawler:
            return self.anti_crawler_session.get_stats()
        else:
            return {"message": "é˜²åçˆ¬ç³»ç»Ÿæœªå¯ç”¨"}
    
    def reset_anti_crawler_stats(self):
        """é‡ç½®é˜²åçˆ¬ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if self.use_anti_crawler:
            self.anti_crawler_session.reset_stats()
            self.logger.info("é˜²åçˆ¬ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")

class ScheduledCrawler:
    """å®šæ—¶çˆ¬è™«ç±»ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, crawler: WeChatCrawler):
        self.crawler = crawler
        self.logger = logging.getLogger(__name__)
        self.is_running = False
    
    def schedule_daily_crawl(self, time_str: str = "08:00"):
        """è®¾ç½®æ¯æ—¥æ—©ä¸Š8ç‚¹å®šæ—¶çˆ¬å–ï¼ˆçˆ¬å–å‰ä¸€å¤©ä¸‹åˆ3ç‚¹åˆ°ä»Šå¤©æ—©ä¸Š8ç‚¹çš„å†…å®¹ï¼‰"""
        def job():
            self.logger.info("å¼€å§‹æ¯æ—¥å®šæ—¶çˆ¬å–ä»»åŠ¡")
            
            # è®¡ç®—æ—¶é—´èŒƒå›´ï¼šå‰ä¸€å¤©ä¸‹åˆ3ç‚¹åˆ°ä»Šå¤©æ—©ä¸Š8ç‚¹
            now = datetime.now()
            today_8am = now.replace(hour=8, minute=0, second=0, microsecond=0)
            yesterday_3pm = (now - timedelta(days=1)).replace(hour=15, minute=0, second=0, microsecond=0)
            
            self.logger.info(f"çˆ¬å–æ—¶é—´èŒƒå›´: {yesterday_3pm.strftime('%Y-%m-%d %H:%M:%S')} åˆ° {today_8am.strftime('%Y-%m-%d %H:%M:%S')}")
            
            # è·å–é…ç½®çš„å…¬ä¼—å·åˆ—è¡¨
            accounts = self.crawler.load_wechat_accounts()
            
            if not accounts:
                self.logger.warning("æ²¡æœ‰é…ç½®ä»»ä½•å…¬ä¼—å·ï¼Œè·³è¿‡å®šæ—¶ä»»åŠ¡")
                return
            
            # ä¸ºæ¯ä¸ªå…¬ä¼—å·å•ç‹¬çˆ¬å–å’Œä¿å­˜
            total_articles_saved = 0
            date_str = now.strftime('%Y_%m%d')  # æ ¼å¼ï¼š2025_0905
            
            for account in accounts:
                try:
                    self.logger.info(f"å¼€å§‹çˆ¬å–å…¬ä¼—å·: {account}")
                    
                    # çˆ¬å–å•ä¸ªå…¬ä¼—å·çš„æ–‡ç« 
                    results = self.crawler.crawl_and_extract(
                        query=account,
                        page=1,
                        get_real_urls=True,
                        fetch_content=True,
                        start_time=yesterday_3pm,
                        end_time=today_8am
                    )
                    
                    if results['success'] and results['data']:
                        # å¦‚æœæœ‰æ–‡ç« ï¼Œä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶
                        filename = f'wechat_{account}_{date_str}.json'
                        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
                        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
                        
                        self.crawler.save_results(results, filename)
                        
                        article_count = len(results['data'])
                        total_articles_saved += article_count
                        
                        self.logger.info(f"å…¬ä¼—å· '{account}' çˆ¬å–å®Œæˆ: {article_count} ç¯‡æ–‡ç« å·²ä¿å­˜åˆ° {filename}")
                    else:
                        self.logger.info(f"å…¬ä¼—å· '{account}' åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æ²¡æœ‰å‘å¸ƒæ–‡ç« ")
                        
                except Exception as e:
                    self.logger.error(f"çˆ¬å–å…¬ä¼—å· '{account}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    continue
                
                # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(random.uniform(3, 8))  # éšæœºå»¶è¿Ÿ3-8ç§’
            
            self.logger.info(f"æ¯æ—¥å®šæ—¶ä»»åŠ¡å®Œæˆ: å…±å¤„ç† {len(accounts)} ä¸ªå…¬ä¼—å·ï¼Œä¿å­˜ {total_articles_saved} ç¯‡æ–‡ç« ")
        
        # è®¾ç½®æ¯æ—¥å®šæ—¶ä»»åŠ¡
        schedule.every().day.at(time_str).do(job)
        self.logger.info(f"å·²è®¾ç½®æ¯æ—¥å®šæ—¶ä»»åŠ¡: æ¯å¤©{time_str}çˆ¬å–æ‰€æœ‰é…ç½®çš„å…¬ä¼—å·ï¼ˆå‰ä¸€å¤©15:00åˆ°å½“å¤©08:00çš„å†…å®¹ï¼‰")
    
    def run_daily_crawl_now(self):
        """ç«‹å³æ‰§è¡Œæ¯æ—¥çˆ¬å–ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        self.logger.info("æ‰‹åŠ¨è§¦å‘æ¯æ—¥çˆ¬å–ä»»åŠ¡")
        
        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼šå‰ä¸€å¤©ä¸‹åˆ3ç‚¹åˆ°ä»Šå¤©æ—©ä¸Š8ç‚¹
        now = datetime.now()
        today_8am = now.replace(hour=8, minute=0, second=0, microsecond=0)
        yesterday_3pm = (now - timedelta(days=1)).replace(hour=15, minute=0, second=0, microsecond=0)
        
        self.logger.info(f"çˆ¬å–æ—¶é—´èŒƒå›´: {yesterday_3pm.strftime('%Y-%m-%d %H:%M:%S')} åˆ° {today_8am.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # è·å–é…ç½®çš„å…¬ä¼—å·åˆ—è¡¨
        accounts = self.crawler.load_wechat_accounts()
        
        if not accounts:
            self.logger.warning("æ²¡æœ‰é…ç½®ä»»ä½•å…¬ä¼—å·ï¼Œè·³è¿‡çˆ¬å–ä»»åŠ¡")
            return 0
        
        # ä¸ºæ¯ä¸ªå…¬ä¼—å·å•ç‹¬çˆ¬å–å’Œä¿å­˜
        total_articles_saved = 0
        date_str = now.strftime('%Y_%m%d')  # æ ¼å¼ï¼š2025_0905
        
        for account in accounts:
            try:
                self.logger.info(f"å¼€å§‹çˆ¬å–å…¬ä¼—å·: {account}")
                
                # çˆ¬å–å•ä¸ªå…¬ä¼—å·çš„æ–‡ç« 
                results = self.crawler.crawl_and_extract(
                    query=account,
                    page=1,
                    get_real_urls=True,
                    fetch_content=True,
                    start_time=yesterday_3pm,
                    end_time=today_8am
                )
                
                if results['success'] and results['data']:
                    # å¦‚æœæœ‰æ–‡ç« ï¼Œä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶
                    filename = f'wechat_{account}_{date_str}.json'
                    # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
                    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
                    
                    self.crawler.save_results(results, filename)
                    
                    article_count = len(results['data'])
                    total_articles_saved += article_count
                    
                    self.logger.info(f"å…¬ä¼—å· '{account}' çˆ¬å–å®Œæˆ: {article_count} ç¯‡æ–‡ç« å·²ä¿å­˜åˆ° {filename}")
                else:
                    self.logger.info(f"å…¬ä¼—å· '{account}' åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æ²¡æœ‰å‘å¸ƒæ–‡ç« ")
                    
            except Exception as e:
                self.logger.error(f"çˆ¬å–å…¬ä¼—å· '{account}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
            
            # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(random.uniform(3, 8))  # éšæœºå»¶è¿Ÿ3-8ç§’
        
        self.logger.info(f"æ‰‹åŠ¨çˆ¬å–ä»»åŠ¡å®Œæˆ: å…±å¤„ç† {len(accounts)} ä¸ªå…¬ä¼—å·ï¼Œä¿å­˜ {total_articles_saved} ç¯‡æ–‡ç« ")
        return total_articles_saved
    
    def run_scheduler(self):
        """è¿è¡Œè°ƒåº¦å™¨"""
        self.is_running = True
        self.logger.info("å®šæ—¶çˆ¬è™«è°ƒåº¦å™¨å¯åŠ¨")
        
        while self.is_running:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    
    def stop_scheduler(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.is_running = False
        self.logger.info("å®šæ—¶çˆ¬è™«è°ƒåº¦å™¨å·²åœæ­¢")

def main():
    """ä¸»å‡½æ•°ï¼šç®€åŒ–ç‰ˆ"""
    print("=== å¾®ä¿¡å…¬ä¼—å·çˆ¬è™« ===")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = WeChatCrawler()
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½å…¬ä¼—å·åˆ—è¡¨
    accounts = crawler.load_wechat_accounts()
    print(f"\nå·²åŠ è½½ {len(accounts)} ä¸ªå…¬ä¼—å·")
    
    # é€‰æ‹©çˆ¬å–æ–¹å¼
    print("\né€‰æ‹©åŠŸèƒ½:")
    print("1. æ™®é€šçˆ¬å–ï¼ˆæ‰€æœ‰é…ç½®çš„å…¬ä¼—å·ï¼‰")
    print("2. æµ‹è¯•å®šæ—¶çˆ¬å–ï¼ˆå‰ä¸€å¤©15:00-ä»Šå¤©08:00ï¼ŒæŒ‰å…¬ä¼—å·åˆ†åˆ«ä¿å­˜ï¼‰")
    print("3. å¯åŠ¨å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©08:00è‡ªåŠ¨æ‰§è¡Œï¼‰")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3ï¼Œé»˜è®¤2): ").strip() or "2"
    
    if choice == "1":
        # æ™®é€šçˆ¬å–æ‰€æœ‰é…ç½®çš„å…¬ä¼—å·
        print(f"\nå¼€å§‹çˆ¬å–æ‰€æœ‰å…¬ä¼—å·...")
        results = crawler.crawl_all_configured_accounts(get_real_urls=True, fetch_content=True)
        
        # ä¿å­˜ç»“æœ
        filename = crawler.save_results(results)
        print(f"\nçˆ¬å–å®Œæˆï¼")
        print(f"ç»Ÿè®¡: æ‰¾åˆ° {results['stats']['total_found']} ç¯‡æ–‡ç« ")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        
    elif choice == "2":
        # æµ‹è¯•æ¯æ—¥å®šæ—¶çˆ¬å–
        print("\nå¼€å§‹æµ‹è¯•å®šæ—¶çˆ¬å–...")
        print("æ—¶é—´èŒƒå›´ï¼šå‰ä¸€å¤©15:00 åˆ° ä»Šå¤©08:00")
        print("æ–‡ä»¶æ ¼å¼ï¼šwechat_å…¬ä¼—å·åç§°_2025_0905.json")
        
        scheduled_crawler = ScheduledCrawler(crawler)
        total_saved = scheduled_crawler.run_daily_crawl_now()
        
        print(f"\næµ‹è¯•å®Œæˆï¼å…±ä¿å­˜ {total_saved} ç¯‡æ–‡ç« ")
        
    elif choice == "3":
        # å¯åŠ¨å®šæ—¶ä»»åŠ¡
        print("\nå¯åŠ¨å®šæ—¶ä»»åŠ¡...")
        print("æ¯å¤©æ—©ä¸Š08:00è‡ªåŠ¨çˆ¬å–å‰ä¸€å¤©15:00-å½“å¤©08:00çš„æ–‡ç« ")
        print("æŒ‰ Ctrl+C åœæ­¢")
        
        scheduled_crawler = ScheduledCrawler(crawler)
        scheduled_crawler.schedule_daily_crawl("08:00")
        
        try:
            scheduled_crawler.run_scheduler()
        except KeyboardInterrupt:
            print("\nå®šæ—¶ä»»åŠ¡å·²åœæ­¢")
            scheduled_crawler.stop_scheduler()
            
    else:
        print("æ— æ•ˆé€‰æ‹©")
        return
    
    print(f"\nğŸ“ æç¤º: è¦ä¿®æ”¹å…¬ä¼—å·åˆ—è¡¨ï¼Œè¯·ç¼–è¾‘ {crawler.config_file} æ–‡ä»¶")

if __name__ == "__main__":
    main()
